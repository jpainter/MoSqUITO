---
title: "Untitled"
author: "jp"
date: "6/5/2019"
output: html_document
params:
  origin: Sierra Leone
  origin_login_file: sierra leone
  destination_login_file: local_sierra_leone
  cache: FALSE
editor_options: 
  chunk_output_type: console
---
# Setup 

load libraries...

```{r packages, echo = TRUE, results='hold', message=FALSE }

# list of required packages
    package.list = c("rlist", "knitr", "kableExtra", "xts", "leaflet", "RColorBrewer", "DT", "dygraphs", "httr", "jsonlite", "XML", "assertthat", "lubridate", "scales", "RcppRoll", "zoo", "gridExtra", "futile.logger", "tidyverse", "anytime" )

# Function to test if package is installed 
    pkgTest <- function( package.list = package.list ){
        
        missing.packages = setdiff( package.list , rownames(installed.packages())) 
        if ( length( missing.packages ) > 0 ) install.packages( missing.packages ) 
    }


# Test if packages loaded
    pkgTest( package.list )

# load the packages
    lapply( package.list, suppressMessages( require ) , character.only = TRUE)


knitr::opts_chunk$set( echo = params$echo ,
                       fig.width =  8, 
                       # knitr.table.format = "html" ,
                       cache = params$cache # TRUE for testing; FALSE for production
                       )

```

and functions...

```{r sources }
source('../HMIS/DHIS2/dhis2-data-munging/bootstrap/bootstrap_functions.R')

source('../HMIS/Surv_Assessment/dhis2_functions.R')

origin.folder = paste0( "../HMIS/Surv_Assessment/" , 
                        params$origin, "/" )
 # login shortcuts
login_destination = function(){
    source( 
    paste0( origin.folder, 
            tolower( params$destination_login_file ) , "_login" )
    )

  loginDHIS2( baseurl, username, password)
}

login_destination()
```


Open list of SL admin boundaries

```{r}
 meta_data_file = paste0( origin.folder , "datasets/" ,
                           params$origin , "_metadata.rds" ) 

  if ( file.exists( meta_data_file) ){
   
      md = read_rds( meta_data_file )
  }

 district.level = md$organisationUnitLevels %>% filter( displayName %in% 'District' ) %>% pull(level )

 districts = md$organisationUnits %>%
   filter( level %in% district.level ) %>%
   dplyr::select( id, name ) %>%
   rename( districts = name ) %>%
   mutate( districts = gsub( " District" , "", districts ))
 
 districts

```

# Convert administrative areas to simple feature file


```{r ous_from_metatdata }

# file with org units as SF 
  ous_file =  paste0( origin.folder , "datasets/" ,
                           params$origin , "_ous.rds" ) 

if ( file.exists( ous_file )){
  
  ous.sf = readRDS( ous_file )
  
  # test plot
  # plot( ous.sf[, c('orgUnit.name', 'geometry')] )
  
} else {
  
  ous.sf = ous_from_metatdata( .meta = md , simplify = FALSE , SF = TRUE ) 
  
  saveRDS( ous.sf , ous_file )
}
 
       
```


# Use admin boundaries to summarise arc2 data

```{r summarise_over_admin }

    source = 'ARC2'

    # lowest level with administrative boundaries
    adm_level = ous.sf$level[ ous.sf$feature %in% 'Polygon' ] %>% max()


    arc2_adm_file = paste0( "Datasets/", source, 
                           "_adm_level", adm_level ,
                           ".rds")
   
   if ( file.exists( arc2_adm_file )){ 
     
            arc2_adm = read_rds( arc2_adm_file )
       
   } else { 

    source(  "Datasets/ARC_rainfall_by_adm.R")
    
    source.data.directory = '../ARC/data/arc/'

    arc2_adm = adm_rainfall( adm = adm_level , 
                            years = 2000:2019 ,
                            admin.sf = ous.sf ,
                            data.directory =  source.data.directory ,
                            source = source )
    


   saveRDS( arc2_adm , 
            file = arc2_adm_file )
   
   }
   
  glimpse( arc2_adm )
```

## Confirm all districts have data 

```{r}

admins =  ous.sf %>% 
  filter( feature %in% 'Polygon' ) 
  # st_set_geometry(NULL) %>% 
  # dplyr::select( orgUnit, level.name , parent_ou.name , feature , geometry ) 


d = arc2_adm %>% inner_join( admins  ,
                             by = 'orgUnit' )

count( d, level.name )

count( d, parent_ou.name )

```

Lets review data values to see if the values look reasonable.  Sum over districts

```{r box_plot}

parent = d %>% group_by( parent_ou.name, period ) %>% summarise( rain = mean( rain_mean , na.rm = T))

summary( parent$rain )

ggplot( parent, aes( parent_ou.name , rain ) ) + 
  geom_boxplot() +
  coord_flip()

parent %>% filter( parent_ou.name %in% 'Kambia District') %>%
  dplyr::select( period, rain, parent_ou.name ) %>%
  ggplot( aes( period, rain, group = parent_ou.name )) + geom_line()

# check out chiefdoms within
d %>% filter( parent_ou.name %in% 'Kambia District') %>%
  dplyr::select( period, rain_mean, orgUnit.name ) %>%
  ggplot( aes( period, rain_mean, group = orgUnit.name , color = orgUnit.name)) + geom_line()

# Samu Chiefdom 
d %>% filter( orgUnit.name %in% 'Samu Chiefdom') %>%
  dplyr::select( period, rain_mean, orgUnit.name ) %>%
  arrange( -rain_mean )


# ggplot( ous.sf , aes( fill = orgUnit.name ) ) + geom_sf( )

```

# Use HF area (estimated by Voronoi) to summarise ARC2 

## estimate catchment areas
```{r voronoi}

hfs =  ous.sf %>% filter( feature %in% 'Point') 

# convert to a multipolygon
hfs.multipoly = st_union( hfs )
head(hfs.multipoly)

national.border = st_union( st_buffer( admins %>% filter( level == 2 ) , dist = 0)  )

v <- st_voronoi( hfs.multipoly , national.border )

# clip to national border
hfc = st_intersection( st_cast(v), national.border )

# test:
plot( hfc, col = 0)  # voronoi witihin bounding box
plot( hfs, add = TRUE )

hfc.file =  "hfc_voronoi.rds"
saveRDS( hfc , hfc.file ) 


```

## Summarise data

```{r summarise_over_hfc}
   source = 'ARC2'

    adm_level = "hfc"
    
    arc2_hfc_file = paste0( "Datasets/", source, 
                           "_adm_level", adm_level ,
                           ".rds")
   
   if ( file.exists( arc2_hfc_file )){ 
     
            arc2_hfc = read_rds( arc2_adm_file )
       
   } else { 

    source(  "Datasets/ARC_rainfall_by_adm.R")
    
    source.data.directory = '../ARC/data/arc/'

    arc2_hfc = adm_rainfall( adm = adm_level , 
                            years = 2000:2019 ,
                            admin.sf = hfc ,
                            data.directory =  source.data.directory ,
                            source = source )
    


   saveRDS( arc2_hfc , 
            file = arc2_hfc_file )
   
   }
   
  glimpse( arc2_hfc )
```



# Import

Create a data element for ARC2 

```{r}
set.seed(94005004)
arc2.id = generateUID()

arc_de = tibble( name = "ARC2 mean rainfall" , 
                 shortName = "Mean Rain",
                 domainType = "AGGREGATE" , 
                 valueType = "NUMBER" , 
                 aggregationType = "AVERAGE" ,
                 categoryCombo = data.frame( id = "AkqpoipjtNc" , stringsAsFactors = FALSE) , #default_#
                 zeroIsSignificant = TRUE ,
                 id = arc2.id )


 login_destination()

 url <- paste0( baseurl , 
                 "api/metadata?importStrategy=CREATE_AND_UPDATE&atomicMode=NONE"
                )
     

 de.list = list( dataElements = arc_de )

 post( url, de.list ) 
  
```


And, a new data element group, 'NASA'

<!-- TODO: FIX-- -->

```{r}
set.seed(94005005)
degroup.id = generateUID()



nasa_degp = tibble( name = "NASA" , 
                 id = degroup.id ,
                 dataElements = list(data.frame( id = arc2.id)) 
                 )



 login_destination()

 url <- paste0( baseurl , 
                 "api/metadata?importStrategy=CREATE_AND_UPDATE&atomicMode=NONE"
                )
     

   degrp.list = list( dataElementGroups = nasa_degp )
  
  post( url, degrp.list )
  
```

Now, prepare the dataset

```{r}
 data = arc2_adm %>% 
  mutate( value = as.character( rain_mean ) ,
          dataElement = arc2.id 
          # Fixed period format (padding mnth) when creating arc2_adm, so below deprecated
          # , year = substr( period , 1, 4 ) 
          # , month = substr( period , 5, length(period) ) 
          # # note: need to convert month to text and pad  so that always 2 characters
          # , period = paste0( year, str_pad( month , 2, pad = "0")  ) 
          ) %>%
  dplyr::select( orgUnit, dataElement ,  period , value )

glimpse( data )
```

And then ..

- After getting the data into shape, import it.

```{r import_data , echo = TRUE}

  url <- paste0( baseurl,
                 "api/dataValueSets?preheatCache=false&skipExistingCheck=false" )
  
# Import month by month
  periods = unique( data$period ) 
  periods = periods[ order( periods )]
  
for ( i in seq_along(periods)  ){ 
  
  print( paste( periods[i] ) )
  
  d_out = data %>% 
    filter( period %in% periods[i] ) 
  
  p = POST( url, 
              body = toJSON( list( dataValues = d_out ) ,
                             auto_unbox = TRUE ) ,
              content_type_json()
    )
  
  # stop if did not import successfully
  status = fromJSON(content(p, "text" ))$description
  if ( !status %in% "Import process completed successfully" ){
    print( 'Import error')
    break
  } 
  
  # results
  print( fromJSON(content(p, "text" ))$importCount %>% data.frame() ) 
}

  
```

# Trigger analytics


```{r trigger_analytics }


  login_destination()

  start = Sys.time()
  
  url <- paste0(baseurl, "api/resourceTables/analytics" ) 
  r <- POST(url)
  assert_that (r$status_code == 200L )
  
```


```{r waiting_to_finish , echo = FALSE, results = 'hide' }

## Warning--'connection refused' happens here sometimes.  Why???

completed <- FALSE

start = Sys.time()

cat( 'started analytics at' , as.character( ymd_hms( start ) ) )
 
cat( " Waiting ")
while ( completed == FALSE ) { 
  
  Sys.sleep(5)
  
  r = get( paste0(baseurl,"api/system/tasks/ANALYTICS_TABLE"), 
           flatten = TRUE , .print = FALSE )
  
  cat( "." )
  
  completed <- r[ 1, "completed" ] 
  
}

  stop = Sys.time()
  
 cat( 'started analytics at' , as.character( ymd_hms( start ) ) )
 
 cat( 'Analytics ended' , as.character( ymd_hms( stop ) ) )
 
 difftime( stop, start, units = 'mins')

```

